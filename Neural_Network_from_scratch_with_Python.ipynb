{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-layer Neural Network for Classification \n",
    "without the deep learning framework (only python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.  Import dependency package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "The following functions are retrieved from https://stackoverflow.com/questions/40427435/extract-images-from-idx3-ubyte-file-or-gzip-via-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_images():\n",
    "    with gzip.open('data/train-images-idx3-ubyte.gz', 'r') as f:\n",
    "        # first 4 bytes is a magic number\n",
    "        magic_number = int.from_bytes(f.read(4), 'big')\n",
    "        # second 4 bytes is the number of images\n",
    "        image_count = int.from_bytes(f.read(4), 'big')\n",
    "        # third 4 bytes is the row count\n",
    "        row_count = int.from_bytes(f.read(4), 'big')\n",
    "        # fourth 4 bytes is the column count\n",
    "        column_count = int.from_bytes(f.read(4), 'big')\n",
    "        # rest is the image pixel data, each pixel is stored as an unsigned byte\n",
    "        # pixel values are 0 to 255\n",
    "        image_data = f.read()\n",
    "        images = np.frombuffer(image_data, dtype=np.uint8)\\\n",
    "            .reshape((image_count, row_count, column_count))\n",
    "        return images\n",
    "\n",
    "\n",
    "def training_labels():\n",
    "    with gzip.open('data/train-labels-idx1-ubyte.gz', 'r') as f:\n",
    "        # first 4 bytes is a magic number\n",
    "        magic_number = int.from_bytes(f.read(4), 'big')\n",
    "        # second 4 bytes is the number of labels\n",
    "        label_count = int.from_bytes(f.read(4), 'big')\n",
    "        # rest is the label data, each label is stored as unsigned byte\n",
    "        # label values are 0 to 9\n",
    "        label_data = f.read()\n",
    "        labels = np.frombuffer(label_data, dtype=np.uint8)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = training_images()\n",
    "Y_t = training_labels()\n",
    "\n",
    "X_t = X_t.reshape(-1,784)\n",
    "\n",
    "# normalization\n",
    "X_t = X_t / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Neural Network\n",
    "\n",
    "### 2.0 Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def prop(self, X):\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def derivative(self, X):\n",
    "        result = np.copy(X)\n",
    "        result[result >= 0] = 1\n",
    "        result[result < 0] = 0\n",
    "        return result\n",
    "\n",
    "class LeakyReLU:\n",
    "    def __init__(self, hyper):\n",
    "        self.hyper = hyper\n",
    "        \n",
    "    def prop(self, X):\n",
    "        return np.maximum(self.hyper*X, X)\n",
    "    \n",
    "    def derivative(self, X):\n",
    "        result = np.copy(X)\n",
    "        result[result >= 0] = 1\n",
    "        result[result < 0] = self.hyper\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Layer_Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_dense:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.W = np.random.normal(size=(input_size, output_size))\n",
    "        self.b = np.random.normal(size=(1,output_size))\n",
    "        self.L = None\n",
    "        self.activation_func = activation()\n",
    "    \n",
    "    def prop(self, X):\n",
    "        self.L = np.dot(X, self.W) + self.b\n",
    "        return self.activation_func.prop(self.L)\n",
    "    \n",
    "    def derivative(self):\n",
    "        return self.W\n",
    "    \n",
    "    def activation_derivative(self):\n",
    "        return self.activation_func.derivative(self.L)\n",
    "    \n",
    "    def update(self, learning_rate, dW, db):\n",
    "        self.W -= learning_rate * dW\n",
    "        self.b -= learning_rate * db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def prop(self, X):\n",
    "        return np.exp(X)/np.sum(np.exp(X), axis=0)\n",
    "    \n",
    "    def derivative(self, Y_hat, Y):\n",
    "        \"I will use this function for derivative loss_func(softmax(Y_hat),Y)\"\n",
    "        result = np.copy(Y_hat)\n",
    "        result[range(result.shape[0]),Y] -= 1\n",
    "        return result \n",
    "    \n",
    "    def _derivative(self, X):\n",
    "        \"\"\"I will not use this function, just implementation of partial derivative\"\"\"\n",
    "        result = np.zeros((*X.shape,X.shape[-1]))\n",
    "        S = Softmax.prop(X)\n",
    "        for k, V in enumerate(X):\n",
    "            for i,x1 in enumerate(*V):\n",
    "                for j,x2 in enumerate(*V):\n",
    "                    if i==j:\n",
    "                        result[k,i,j] = S[k,i] * (1 - S[k,j])\n",
    "                    else:\n",
    "                        result[k,i,j] = S[k,i] * S[k,j] * -1\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Multi-Class Cross entropy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_entropy:\n",
    "    def prop(self, Y_hat, Y):\n",
    "        return -1 * np.sum(np.log(Y_hat[range(Y_hat.shape[0]),Y]))\n",
    "    \n",
    "\n",
    "    def derivative(self, Y_hat, Y):\n",
    "        \"I will not use this function, just implementation of derivative\"\n",
    "        result = np.zeros(Y_hat.shape)\n",
    "        result[range(Y_hat.shape[0]),Y] = -1 * (1/Y_hat[range(Y_hat.shape[0]),Y])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, X_t, Y_t, loss, learning_rate, batch_size, epoch):\n",
    "        self.X_t = X_t\n",
    "        self.Y_t = Y_t\n",
    "        \n",
    "        self.loss_func = loss()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = epoch\n",
    "        \n",
    "        self.layers = []\n",
    "        self.inputs = []\n",
    "\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def prop(self, X):\n",
    "        result = X\n",
    "        self.inputs = [X]\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            result = layer.prop(result)\n",
    "            self.inputs.append(result)\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def backprop(self, Y):\n",
    "        dZ = self.layers[-1].derivative(self.inputs[-1], Y) # softmax+cost \n",
    "        for i,layer in enumerate(reversed(self.layers[:-1]),3):\n",
    "            dZ = np.multiply(layer.activation_derivative(), dZ)\n",
    "            dW = np.dot(self.inputs[-i].T,dZ) / self.batch_size\n",
    "            db = np.sum(dZ, axis=0, keepdims = True) / self.batch_size\n",
    "            dZ = np.dot(dZ,layer.derivative().T)\n",
    "            layer.update(self.learning_rate, dW, db)\n",
    "    \n",
    "    def get_loss(self, Y):\n",
    "        return self.loss_func.prop(self.inputs[-1],Y) / self.batch_size\n",
    "    \n",
    "    def fit(self, verbose=True):\n",
    "        \n",
    "        div = int(self.X_t.shape[0] / self.batch_size)\n",
    "        mod = self.X_t.shape[0] % self.batch_size\n",
    "        n = div if mod < 100 else div + 1\n",
    "        \n",
    "        for i in range(self.epoch):\n",
    "            for j,X,Y in self.batch_iter(div,mod,n):\n",
    "                self.prop(X)\n",
    "                self.backprop(Y)\n",
    "#                 if verbose:\n",
    "#                     print(f\"[{j}/{n}] cost: {self.get_loss(Y)}\")\n",
    "            if verbose:\n",
    "                print(f\"[{i}/{self.epoch}] cost: {self.get_loss(Y)}\")\n",
    "\n",
    "    \n",
    "    def batch_iter(self,div,mod,n):\n",
    "        for i in range(n):\n",
    "            if i == n - 1:\n",
    "                yield i, self.X_t[i*self.batch_size:(i+1)*self.batch_size+mod+1], self.Y_t[i*self.batch_size:(i+1)*self.batch_size+mod+1]\n",
    "            else:\n",
    "                yield i, self.X_t[i*self.batch_size:(i+1)*self.batch_size], self.Y_t[i*self.batch_size:(i+1)*self.batch_size]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        for layer in layers:\n",
    "            X = layer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_t, Y_t, loss, learning_rate, batch_size, epoch\n",
    "nn = NN(X_t, Y_t, Cross_entropy, 0.01, 256, 1000)\n",
    "nn.add(Layer_dense(784,64,ReLU))\n",
    "nn.add(Layer_dense(64,10,ReLU))\n",
    "nn.add(Softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-919cbf48e25e>:3: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(X)/np.sum(np.exp(X), axis=0)\n",
      "<ipython-input-7-919cbf48e25e>:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(X)/np.sum(np.exp(X), axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1000] cost: nan\n",
      "[1/1000] cost: nan\n",
      "[2/1000] cost: nan\n",
      "[3/1000] cost: nan\n",
      "[4/1000] cost: nan\n",
      "[5/1000] cost: nan\n",
      "[6/1000] cost: nan\n",
      "[7/1000] cost: nan\n",
      "[8/1000] cost: nan\n",
      "[9/1000] cost: nan\n",
      "[10/1000] cost: nan\n",
      "[11/1000] cost: nan\n",
      "[12/1000] cost: nan\n",
      "[13/1000] cost: nan\n",
      "[14/1000] cost: nan\n",
      "[15/1000] cost: nan\n",
      "[16/1000] cost: nan\n",
      "[17/1000] cost: nan\n",
      "[18/1000] cost: nan\n",
      "[19/1000] cost: nan\n",
      "[20/1000] cost: nan\n",
      "[21/1000] cost: nan\n",
      "[22/1000] cost: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-94af84bd4965>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-7fc9a984203b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;31m#                 if verbose:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#                     print(f\"[{j}/{n}] cost: {self.get_loss(Y)}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-7fc9a984203b>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, Y)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mdZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mdZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mderivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trivial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(10).reshape(5,-1)\n",
    "b = np.array([1,2])\n",
    "np.multiply(a,b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
