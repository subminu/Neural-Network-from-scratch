{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-layer Neural Network for Classification \n",
    "without the deep learning framework (only python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.  Import dependency package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "The following functions are retrieved from https://stackoverflow.com/questions/40427435/extract-images-from-idx3-ubyte-file-or-gzip-via-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_images():\n",
    "    with gzip.open('data/train-images-idx3-ubyte.gz', 'r') as f:\n",
    "        # first 4 bytes is a magic number\n",
    "        magic_number = int.from_bytes(f.read(4), 'big')\n",
    "        # second 4 bytes is the number of images\n",
    "        image_count = int.from_bytes(f.read(4), 'big')\n",
    "        # third 4 bytes is the row count\n",
    "        row_count = int.from_bytes(f.read(4), 'big')\n",
    "        # fourth 4 bytes is the column count\n",
    "        column_count = int.from_bytes(f.read(4), 'big')\n",
    "        # rest is the image pixel data, each pixel is stored as an unsigned byte\n",
    "        # pixel values are 0 to 255\n",
    "        image_data = f.read()\n",
    "        images = np.frombuffer(image_data, dtype=np.uint8)\\\n",
    "            .reshape((image_count, row_count, column_count))\n",
    "        return images\n",
    "\n",
    "\n",
    "def training_labels():\n",
    "    with gzip.open('data/train-labels-idx1-ubyte.gz', 'r') as f:\n",
    "        # first 4 bytes is a magic number\n",
    "        magic_number = int.from_bytes(f.read(4), 'big')\n",
    "        # second 4 bytes is the number of labels\n",
    "        label_count = int.from_bytes(f.read(4), 'big')\n",
    "        # rest is the label data, each label is stored as unsigned byte\n",
    "        # label values are 0 to 9\n",
    "        label_data = f.read()\n",
    "        labels = np.frombuffer(label_data, dtype=np.uint8)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = training_images()\n",
    "Y_t = training_labels()\n",
    "\n",
    "X_t = X_t.reshape(-1,1,784)\n",
    "\n",
    "# normalization\n",
    "X_t = X_t / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Neural Network\n",
    "\n",
    "### 2.0 Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def prop(self, X):\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def derivative(self, X):\n",
    "        result = np.copy(X)\n",
    "        result[result >= 0] = 1\n",
    "        result[result < 0] = 0\n",
    "        return result\n",
    "\n",
    "class LeakyReLU:\n",
    "    def __init__(self, hyper):\n",
    "        self.hyper = hyper\n",
    "        \n",
    "    def prop(self, X):\n",
    "        return np.maximum(self.hyper*X, X)\n",
    "    \n",
    "    def derivative(self, X):\n",
    "        result = np.copy(X)\n",
    "        result[result >= 0] = 1\n",
    "        result[result < 0] = self.hyper\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Layer_Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_dense:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.W = np.random.normal(size=(input_size, output_size))\n",
    "        self.b = np.random.normal(size=(1,output_size))\n",
    "        self.L = None\n",
    "        self.activation_func = activation()\n",
    "    \n",
    "    def prop(self, X):\n",
    "        self.L = np.dot(X, self.W) + self.b\n",
    "        return self.activation_func.prop(self.L)\n",
    "    \n",
    "    def derivative(self, X):\n",
    "        return np.array([ np.multiply(self.W, v) for v in self.activation_func.derivative(X) ])\n",
    "    \n",
    "    def activation_derivative(self):\n",
    "        return self.activation_func.derivative(self.L)\n",
    "    \n",
    "    def update(self, learning_rate, dW, db):\n",
    "        self.W -= learning_rate * dW\n",
    "        self.b -= learning_rate * db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def prop(self, X):\n",
    "        return np.array([ np.exp(x)/np.sum(np.exp(x)) for x in X ])\n",
    "    \n",
    "    def derivative(self, Y_hat, Y):\n",
    "        \"I will use this function for derivative\"\n",
    "        result = np.copy(Y_hat)\n",
    "        result[range(result.shape[0]),0,Y] -= 1\n",
    "        return result \n",
    "    \n",
    "    def _derivative(self, X):\n",
    "        \"\"\"I will not use this function, just implementation of partial derivative\"\"\"\n",
    "        result = np.zeros((*X.shape,X.shape[-1]))\n",
    "        S = Softmax.prop(X)\n",
    "        for k, V in enumerate(X):\n",
    "            for i,x1 in enumerate(*V):\n",
    "                for j,x2 in enumerate(*V):\n",
    "                    if i==j:\n",
    "                        result[k,0,i,j] = S[k,0,i] * (1 - S[k,0,j])\n",
    "                    else:\n",
    "                        result[k,0,i,j] = S[k,0,i] * S[k,0,j] * -1\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Multi-Class Cross entropy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_entropy:\n",
    "    def prop(self, Y_hat, Y):\n",
    "        return -1 * np.log(np.fromiter([ y_hat[:,y] for y_hat,y in zip(Y_hat,Y) ], float))\n",
    "    \n",
    "\n",
    "    def derivative(self, Y_hat, Y):\n",
    "        \"I will not use this function, just implementation of derivative\"\n",
    "        result = np.zeros(Y_hat.shape)\n",
    "        result[range(Y_hat.shape[0]),0,Y] = -1 * (1/Y_hat[range(Y_hat.shape[0]),0,Y])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, X_t, Y_t, loss, learning_rate, batch_size):\n",
    "        self.X_t = X_t\n",
    "        self.Y_t = Y_t\n",
    "        \n",
    "        self.loss_func = loss\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.layers = []\n",
    "        self.inputs = []\n",
    "\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def prop(self, X):\n",
    "        result = X\n",
    "        self.inputs = [X]\n",
    "        for layer in self.layers:\n",
    "            result = layer.prop(result)\n",
    "            self.inputs.append(result)\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def backprop(self, Y):\n",
    "        dZ = self.layers[-1].derivative(self.inputs[-1], Y) # softmax+cost \n",
    "        for i,layer in enumerate(reversed(self.layers),3):\n",
    "            dW = np.multiply(np.dot(self.inputs[-i].T,layer.activation_derivative()), dZ) \n",
    "            db = np.sum(dZ, axis=1, keepdims = True)\n",
    "            dZ = np.dot(layer.derivative(self.inputs[-i]), dZ)\n",
    "            layer.update(self.learning_rate, dW, db)\n",
    "            \n",
    "    \n",
    "    def fit(self):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        for layer in layers:\n",
    "            X = layer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "nn = NN(X_t, Y_t, Cross_entropy(), 0.001, 64)\n",
    "nn.add(Layer_dense(784,64,ReLU))\n",
    "nn.add(Layer_dense(64,10,ReLU))\n",
    "nn.add(Softmax())\n",
    "nn.prop(X_t[:64])\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Softmax' object has no attribute 'backprop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-03cf9566f4c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-49feac11138c>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, Y)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mdZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# softmax+cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mdZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Softmax' object has no attribute 'backprop'"
     ]
    }
   ],
   "source": [
    "nn.backprop(Y_t[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trivial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer0 = Layer_dense(28*28,64,ReLU)\n",
    "layer1 = Layer_dense(64,10,ReLU)\n",
    "\n",
    "batch_x = X_t[0:64]\n",
    "\n",
    "output0 = layer0.prop(batch_x)\n",
    "output1 = layer1.prop(output0)\n",
    "output2 = Softmax().prop(output1)\n",
    "output3 = Cross_entropy().prop(output2, Y_t[0:64])\n",
    "\n",
    "#             if isinstance(layer, Layer_dense):\n",
    "#                 print(\"nn-bakcprop-layer\", i, self.outputs[-i].shape)\n",
    "#                 layer.update(0.001, delta, self.outputs[-i])\n",
    "#                 delta = np.array(layer.backprop(self.outputs[-i]) * delta)\n",
    "#                 continue\n",
    "#             print(\"nn-backprop\", i, layer.backprop(self.outputs[-i]).shape)\n",
    "#             print(\"nn-backprop-delta\",i , delta.shape)\n",
    "#             delta = np.array([prev_d*next_d for next_d, prev_d in zip(layer.backprop(self.outputs[-i]), delta)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_venv",
   "language": "python",
   "name": "dl_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
